<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-2a29cf49-fc56-4f3f-a971-b046f64a3c82"></attachment><h2>生成边界样本用的自表示</h2><h2>1、启发自:</h2><p>ALOCC</p><p><em>Adversarially learned one-class classifier for novelty detection.&nbsp;</em></p><p><br></p><h2>2、思路：</h2><p>GAN初期生成的样本和真实的样本非常不相似，这些样本用来和生成的正常样本一起来训练。</p><p>通过随机的高斯分布噪声生成的样本属于unseen class的概率远高于属于target class的概率，所以这些生成的实例不足以作为分类器的输入来使用</p><p>因为:(1)生成的样本不是语义上类似于真实图像,和(2)他们非常不同于目标类,而二元分类,样本在边界附近的两个类(例如,支持向量在SVM)是至关重要的。</p><p><br></p><h4>boudary samples definition:</h4><p>在结构上相似，但是在语义上不同。这里是指的与ptk与最终收敛的ptj 的KL散度值很低。<span style="background-color: rgb(255, 255, 0);">语义相似要怎么看（？）</span></p><p><br></p><h2>3、方法：</h2><p>GAN用的是Wasserstein GAN (WGAN)。和GAN类似但是去掉了sigmoid函数，C网络给出的是一个具体的数值而不是概率值，这个分数可以被解释为输入图像的真实程度。换句话说，它度量一个状态(输入)有多好。</p><p><span style="background-color: rgb(255, 255, 0);">后面的 i k 有点懵了。</span></p><p><br></p><p><br></p><p>3、别的知识</p><p>GAN的loss并不能表明生成的样本是在边界之内还是远离正常的样本</p><p><br></p>